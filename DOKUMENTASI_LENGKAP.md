# DOKUMENTASI LENGKAP: PDF VECTOR DB MCP SERVER

## Daftar Isi
1. [Pengenalan](#pengenalan)
2. [Arsitektur Sistem](#arsitektur-sistem)
3. [Struktur Proyek](#struktur-proyek)
4. [Teknologi yang Digunakan](#teknologi-yang-digunakan)
5. [Penjelasan Setiap File](#penjelasan-setiap-file)
6. [Alur Kerja Aplikasi](#alur-kerja-aplikasi)
7. [Design Patterns](#design-patterns)
8. [Integrasi Komponen](#integrasi-komponen)

---

## Pengenalan

### Apa Masalah yang Dipecahkan?

Bayangkan Anda memiliki ratusan dokumen PDF berisi informasi penting - buku pelajaran, manual teknis, dokumentasi proyek, atau paper penelitian. Ketika Anda ingin mencari informasi spesifik, Anda harus:

1. **Membuka satu per satu** dokumen PDF
2. **Menggunakan Ctrl+F** untuk mencari kata kunci
3. **Membaca konteks** di sekitar kata kunci
4. **Mencatat** informasi yang relevan
5. **Mengulang proses** untuk dokumen lain

Proses ini:
- ‚è∞ **Memakan waktu** - bisa berjam-jam untuk banyak dokumen
- üòì **Melelahkan** - harus membaca banyak teks
- üéØ **Tidak akurat** - pencarian kata kunci sering melewatkan informasi relevan
- üîÑ **Repetitif** - harus diulang setiap kali butuh informasi

### Solusi: RAG (Retrieval-Augmented Generation)

Aplikasi ini menyelesaikan masalah tersebut dengan:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BEFORE: Manual Search                                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  User ‚îÄ‚îÄ‚ñ∫ Open PDF ‚îÄ‚îÄ‚ñ∫ Ctrl+F ‚îÄ‚îÄ‚ñ∫ Read ‚îÄ‚îÄ‚ñ∫ Take Notes       ‚îÇ
‚îÇ          (manual)      (keyword)  (slow)    (manual)         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Problems: Slow, tiring, keyword-limited, repetitive         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AFTER: Automated RAG System                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  User ‚îÄ‚îÄ‚ñ∫ Ask Question ‚îÄ‚îÄ‚ñ∫ Get Answer with Citations        ‚îÇ
‚îÇ          (natural        (instant,    (automatic)            ‚îÇ
‚îÇ           language)       semantic                           ‚îÇ
‚îÇ                          search)                              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Benefits: Fast, easy, semantic understanding, auto-indexed  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Contoh Penggunaan:

**Pertanyaan:** "Bagaimana cara membuat function di Python?"

**Sistem:**
1. üîç Mencari di semua PDF yang sudah diindeks
2. üéØ Menemukan chunks text yang relevan (berdasarkan makna, bukan kata kunci)
3. üìä Mengurutkan berdasarkan relevansi
4. üìù Memberikan jawaban dengan sumber yang jelas

**Hasil:**
```
Ditemukan 5 chunk relevan:

--- Hasil 1 ---
Sumber: python-intro.pdf (Halaman 45)
Relevansi: 89.3%

Functions di Python didefinisikan dengan keyword def.
Sintaks: def nama_fungsi(parameter):
    # kode di sini
    return hasil

--- Hasil 2 ---
Sumber: python-intro.pdf (Halaman 46)
Relevansi: 85.7%

Contoh function sederhana:
def sapa(nama):
    return f"Halo, {nama}!"
...
```

### Mengapa Aplikasi Ini Penting?

1. **Untuk Pelajar/Mahasiswa**
   - Cari informasi cepat dari buku pelajaran
   - Tidak perlu baca seluruh buku untuk satu topik
   - Dapat citation otomatis untuk referensi

2. **Untuk Developer**
   - Akses cepat ke dokumentasi teknis
   - Tidak perlu ingat semua detail API
   - Temukan contoh code dengan mudah

3. **Untuk Researcher**
   - Search semantic di paper-paper penelitian
   - Temukan paper relevan berdasarkan konsep
   - Ekstrak informasi spesifik dengan cepat

4. **Untuk Enterprise**
   - Knowledge base internal dari dokumen perusahaan
   - Onboarding karyawan baru lebih cepat
   - Compliance dan audit documentation

### Keunggulan Dibanding Solusi Lain

| Fitur | Aplikasi Ini | Cloud RAG (OpenAI) | Manual Search |
|-------|--------------|-------------------|---------------|
| **Biaya** | üí∞ Gratis (100%) | üí∏ Bayar per query | üí∞ Gratis |
| **Privacy** | üîí Data lokal | ‚ö†Ô∏è Upload ke cloud | üîí Data lokal |
| **Kecepatan** | ‚ö° Cepat (lokal) | üåê Tergantung internet | üêå Sangat lambat |
| **Semantic Search** | ‚úÖ Ya | ‚úÖ Ya | ‚ùå Tidak (hanya keyword) |
| **Setup** | üõ†Ô∏è Sekali install | üí≥ Perlu API key | ‚úÖ Tidak perlu |
| **Offline** | ‚úÖ Bisa | ‚ùå Perlu internet | ‚úÖ Bisa |
| **Skalabilitas** | üìà Tergantung hardware | üìà Unlimited | üìâ Tidak scalable |

### Konsep Kunci: Apa itu RAG?

**RAG = Retrieval-Augmented Generation**

Mari kita pecah konsep ini:

1. **Retrieval (Pengambilan)**
   ```
   "Bagaimana cara loop di Python?"
         ‚Üì
   [Cari di database]
         ‚Üì
   Ditemukan text relevan tentang for loop, while loop
   ```

2. **Augmented (Diperkuat)**
   ```
   LLM (Claude) + Context dari PDF
         ‚Üì
   LLM punya informasi tambahan selain knowledge bawaan
   ```

3. **Generation (Pembuatan)**
   ```
   LLM menggunakan context + knowledge
         ‚Üì
   Generate jawaban yang akurat dan relevan
   ```

#### Analogi Sederhana:

**Tanpa RAG:**
```
User: "Berapa harga produk X tahun lalu?"
LLM:  "Maaf, saya tidak punya data spesifik tentang itu."
      (LLM hanya tahu informasi umum)
```

**Dengan RAG:**
```
User: "Berapa harga produk X tahun lalu?"
System: [Cari di database internal] ‚Üí Found: "Produk X: Rp 150.000 (2023)"
LLM:  "Berdasarkan dokumen internal, harga produk X tahun lalu
       adalah Rp 150.000 (Sumber: laporan-2023.pdf, hal 15)"
      (LLM + context dari dokumen)
```

### Konsep Kunci: Vector Embeddings

**Apa itu Embedding?**

Embedding adalah representasi text dalam bentuk angka (vektor) yang menangkap makna semantik.

```
Text:      "Kucing adalah hewan peliharaan"
           ‚Üì [Embedding Model]
Embedding: [0.234, -0.567, 0.123, ..., 0.891]
           (768 angka yang merepresentasikan makna)
```

**Mengapa Perlu Embedding?**

1. **Komputer tidak mengerti text**, hanya mengerti angka
2. **Semantic similarity**: Text dengan makna mirip ‚Üí embedding mirip
3. **Efficient search**: Bisa cari berdasarkan makna, bukan keyword

**Contoh Semantic Similarity:**

```
Text 1: "Mobil saya berwarna merah"
Embed1: [0.1, 0.8, 0.3, ...]

Text 2: "Kendaraan saya berwarna merah"
Embed2: [0.15, 0.75, 0.28, ...]
       ‚Üë Mirip dengan Embed1 (makna sama)

Text 3: "Langit berwarna biru"
Embed3: [0.7, 0.2, 0.9, ...]
       ‚Üë Berbeda dari Embed1 (makna beda)
```

**Cosine Similarity:**

Mengukur seberapa mirip 2 vektor:
```
Similarity(Embed1, Embed2) = 0.92 (92% mirip)
Similarity(Embed1, Embed3) = 0.23 (23% mirip)
```

Ketika user query, sistem:
1. Convert query ‚Üí embedding
2. Bandingkan dengan semua embeddings di database
3. Return text dengan similarity tertinggi

---

## Arsitektur Sistem

### Arsitektur 3-Tier (Berlapis)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PRESENTATION LAYER                          ‚îÇ
‚îÇ              (Interface dengan Claude Desktop)                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  MCP Tools (FastMCP)                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ query_documents()     - Cari di PDF                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ list_documents()      - List semua PDF               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ get_document_info()   - Info detail PDF              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ reindex_document()    - Re-index PDF                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ get_system_stats()    - Statistik sistem             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üï (MCP Protocol)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BUSINESS LOGIC LAYER                        ‚îÇ
‚îÇ              (Processing & Orchestration)                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ pdf_processor‚îÇ  ‚îÇ  embeddings  ‚îÇ  ‚îÇ   file_watcher     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Extract    ‚îÇ  ‚îÇ ‚Ä¢ Generate   ‚îÇ  ‚îÇ ‚Ä¢ Monitor folder   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Chunk      ‚îÇ  ‚îÇ   embeddings ‚îÇ  ‚îÇ ‚Ä¢ Auto-index       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Clean      ‚îÇ  ‚îÇ ‚Ä¢ Batch      ‚îÇ  ‚îÇ ‚Ä¢ Debouncing       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ  ‚îÇ    config    ‚îÇ  ‚îÇ    utils     ‚îÇ                           ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ                           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Settings   ‚îÇ  ‚îÇ ‚Ä¢ Helpers    ‚îÇ                           ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Validation ‚îÇ  ‚îÇ ‚Ä¢ Formatting ‚îÇ                           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üï (Function Calls)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DATA LAYER                                ‚îÇ
‚îÇ              (Storage & Persistence)                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  vector_store.py (Interface ke ChromaDB)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ add_chunks()      - Simpan embeddings                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ query()           - Search semantic                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ delete()          - Hapus dokumen                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ list_documents()  - List semua                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                            ‚Üï                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  ChromaDB (Vector Database)                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ SQLite storage (data/chroma_db/chroma.sqlite3)       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ HNSW index for fast search                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cosine similarity computation                         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                            ‚Üï                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  File System                                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ data/pdfs/        - PDF files                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ data/chroma_db/   - Vector database                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Komponen Eksternal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Claude Desktop (MCP Client)                                 ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  User bertanya ‚Üí Claude ‚Üí MCP Protocol ‚Üí Our Server          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üï (stdio)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PDF Vector DB MCP Server (Aplikasi Kita)                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  [Presentation Layer + Business Logic + Data Layer]          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üï
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Sentence-Transformers Model (all-mpnet-base-v2)            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚Ä¢ Embedding model (768 dimensions)                          ‚îÇ
‚îÇ  ‚Ä¢ Runs locally (CPU/GPU)                                    ‚îÇ
‚îÇ  ‚Ä¢ No API key needed                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Struktur Proyek

### Directory Tree dengan Penjelasan

```
pdf-vectordb-mcp/
‚îÇ
‚îú‚îÄ‚îÄ src/                              # üìÅ Source code utama
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server.py                # üß† OTAK: Orchestration & MCP tools
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # ‚öôÔ∏è Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py             # üìÑ PDF extraction & chunking
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py                # üî¢ Embedding generation (lokal)
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py              # üíæ ChromaDB operations
‚îÇ   ‚îú‚îÄ‚îÄ file_watcher.py              # üëÄ File system monitoring
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                     # üõ†Ô∏è Helper functions
‚îÇ
‚îú‚îÄ‚îÄ data/                             # üìÅ Data storage
‚îÇ   ‚îú‚îÄ‚îÄ pdfs/                        # üìö Input: PDF files di sini
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python-intro.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nodejs-guide.pdf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/                   # üóÑÔ∏è ChromaDB persistence
‚îÇ       ‚îú‚îÄ‚îÄ chroma.sqlite3           # Vector database file
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ tests/                            # üìÅ Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îÇ
‚îú‚îÄ‚îÄ .env                              # üîê Environment variables (local)
‚îú‚îÄ‚îÄ .env.example                      # üìã Template untuk .env
‚îú‚îÄ‚îÄ .gitignore                        # üö´ Files diabaikan git
‚îú‚îÄ‚îÄ requirements.txt                  # üì¶ Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml                    # ‚öôÔ∏è Project configuration
‚îú‚îÄ‚îÄ run_server.py                     # üöÄ Entry point untuk start server
‚îÇ
‚îú‚îÄ‚îÄ README.md                         # üìñ Dokumentasi utama (English)
‚îú‚îÄ‚îÄ QUICKSTART.md                     # ‚ö° Panduan cepat
‚îú‚îÄ‚îÄ ARCHITECTURE.md                   # üèóÔ∏è Dokumentasi arsitektur
‚îú‚îÄ‚îÄ LICENSE                           # ‚öñÔ∏è MIT License
‚îÇ
‚îú‚îÄ‚îÄ DOKUMENTASI_LENGKAP.md           # üìö Dokumentasi lengkap (Bahasa Indonesia)
‚îú‚îÄ‚îÄ MATERI_PENGAJARAN.md             # üë®‚Äçüè´ Materi untuk mengajar
‚îú‚îÄ‚îÄ LATIHAN_PRAKTIKUM.md             # üß™ Lab exercises
‚îÇ
‚îî‚îÄ‚îÄ claude_desktop_config.example.json # üìù Config untuk Claude Desktop
```

### File Sizes & Line Counts (Approximate)

| File | LOC | Purpose |
|------|-----|---------|
| mcp_server.py | ~345 | Server utama & tools |
| pdf_processor.py | ~150 | PDF processing |
| embeddings.py | ~120 | Embedding generation |
| vector_store.py | ~180 | Database operations |
| file_watcher.py | ~90 | File monitoring |
| config.py | ~100 | Configuration |
| utils.py | ~80 | Utilities |
| **TOTAL** | **~1065** | **Full codebase** |

---

## Teknologi yang Digunakan

### 1. FastMCP (v2.13.0.2)

**Apa itu FastMCP?**

FastMCP adalah framework Python untuk membuat MCP (Model Context Protocol) server dengan API yang lebih sederhana.

**Mengapa FastMCP?**

Bandingkan kode dengan MCP SDK asli:

**Dengan MCP SDK (Low-level):**
```python
from mcp.server import Server, Tool
from mcp.types import TextContent

server = Server("my-server")

@server.list_tools()
async def list_tools():
    return [
        Tool(
            name="query_documents",
            description="Search documents",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "top_k": {"type": "integer"}
                }
            }
        )
    ]

@server.call_tool()
async def call_tool(name, arguments):
    if name == "query_documents":
        # Process query
        return [TextContent(type="text", text=result)]
```

**Dengan FastMCP (High-level):**
```python
from fastmcp import FastMCP

mcp = FastMCP("my-server")

@mcp.tool()
def query_documents(query: str, top_k: int = 5) -> str:
    """Search documents"""
    # Process query
    return result  # Langsung return string!
```

**Keuntungan FastMCP:**
- ‚úÖ **85 baris lebih sedikit** (415 ‚Üí 330 lines)
- ‚úÖ **No boilerplate** - langsung fokus ke logic
- ‚úÖ **Auto schema** - dari type hints
- ‚úÖ **Decorator-based** - clean code
- ‚úÖ **Type validation** - otomatis

**Cara Kerja:**
```
@mcp.tool()
    ‚Üì
FastMCP automatically:
  1. Register tool ke server
  2. Generate JSON schema dari type hints
  3. Validate input parameters
  4. Handle serialization/deserialization
  5. Manage MCP protocol communication
```

### 2. ChromaDB (v0.4.0+)

**Apa itu ChromaDB?**

ChromaDB adalah open-source vector database yang bisa di-embed langsung dalam aplikasi Python (tidak perlu server terpisah).

**Konsep: Vector Database**

Database biasa:
```sql
SELECT * FROM documents WHERE title LIKE '%python%'
```
‚Üí Cari berdasarkan keyword exact match

Vector database:
```python
collection.query(
    query_embeddings=[query_vector],
    n_results=5
)
```
‚Üí Cari berdasarkan semantic similarity

**Mengapa ChromaDB?**

| Fitur | ChromaDB | Alternatif (Pinecone, Weaviate) |
|-------|----------|----------------------------------|
| Setup | ‚úÖ Embeddable, no server | ‚ùå Perlu server terpisah |
| Cost | ‚úÖ Gratis | üí∏ Bayar subscription |
| Persistence | ‚úÖ SQLite | üåê Cloud storage |
| Speed | ‚ö° Fast (local) | üåê Tergantung network |
| Similarity | ‚úÖ Cosine, L2, IP | ‚úÖ Multiple metrics |

**Cara Kerja ChromaDB:**

1. **Inisialisasi:**
```python
import chromadb

# Create persistent client
client = chromadb.PersistentClient(path="./data/chroma_db")

# Get or create collection
collection = client.get_or_create_collection(
    name="pdf_documents",
    metadata={"hnsw:space": "cosine"}  # Use cosine similarity
)
```

2. **Simpan Data:**
```python
collection.add(
    ids=["doc1::page_1::chunk_0"],          # Unique ID
    documents=["Python is a language..."],   # Text content
    embeddings=[[0.1, 0.2, ..., 0.8]],      # 768-dim vector
    metadatas=[{"document": "python.pdf", "page": 1}]
)
```

3. **Query:**
```python
results = collection.query(
    query_embeddings=[[0.15, 0.25, ..., 0.75]],  # Query vector
    n_results=5,                                   # Top 5
    where={"document": "python.pdf"}              # Filter (optional)
)
```

**HNSW Index:**

ChromaDB menggunakan HNSW (Hierarchical Navigable Small World) untuk fast approximate nearest neighbor search:

```
Tanpa Index:
  Compare query dengan SEMUA vectors (slow!)
  Time: O(n) dimana n = jumlah vectors

Dengan HNSW:
  Navigate graph structure (fast!)
  Time: O(log n)
  Trade-off: 99% accuracy vs 100x speed
```

### 3. Sentence-Transformers (v2.2.0+)

**Apa itu Sentence-Transformers?**

Library Python untuk generate embeddings dari text menggunakan pre-trained models.

**Model: all-mpnet-base-v2**

Specifications:
- **Architecture:** MPNet (Microsoft)
- **Dimensions:** 768
- **Max tokens:** 384 tokens (~300 words)
- **Model size:** ~420 MB
- **Quality:** Excellent untuk semantic similarity

**Perbandingan Models:**

| Model | Dimensions | Size | Speed | Quality |
|-------|------------|------|-------|---------|
| all-mpnet-base-v2 | 768 | 420MB | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| all-MiniLM-L6-v2 | 384 | 80MB | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê |
| paraphrase-multilingual | 768 | 970MB | Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

**Mengapa all-mpnet-base-v2?**
- ‚úÖ Balance terbaik: quality vs speed vs size
- ‚úÖ Mendukung bahasa Inggris dengan excellent quality
- ‚úÖ 768 dimensions = standard (kompatibel banyak sistem)
- ‚úÖ Mature dan well-tested

**Cara Kerja:**

```python
from sentence_transformers import SentenceTransformer

# Load model (first time: download ~420MB)
model = SentenceTransformer('all-mpnet-base-v2', device='cpu')

# Generate embedding untuk 1 text
text = "Python is a programming language"
embedding = model.encode(text)
# Output: numpy array [768 floats]
# Example: [0.234, -0.567, 0.123, ..., 0.891]

# Batch processing (lebih efisien)
texts = ["text 1", "text 2", "text 3", ...]
embeddings = model.encode(
    texts,
    batch_size=32,           # Process 32 at a time
    show_progress_bar=True   # Show progress
)
# Output: numpy array (n_texts, 768)
```

**Internal Process:**

```
Input Text: "Python is a programming language"
      ‚Üì
1. Tokenization:
   ["[CLS]", "Python", "is", "a", "programming", "language", "[SEP]"]
      ‚Üì
2. BERT-like Transformer:
   - Self-attention layers
   - Feed-forward layers
   - Multiple heads
      ‚Üì
3. Mean Pooling:
   Average all token embeddings
      ‚Üì
4. L2 Normalization:
   Normalize vector to unit length
      ‚Üì
Output: [0.234, -0.567, ..., 0.891]  # 768 floats
```

**CPU vs GPU:**

```
Performance pada 1000 texts:

CPU (Intel i7):
  ‚è±Ô∏è ~60 seconds
  üíæ RAM: ~2GB

GPU (NVIDIA RTX 3060):
  ‚è±Ô∏è ~3 seconds (20x lebih cepat!)
  üíæ VRAM: ~1GB

GPU sangat disarankan untuk indexing banyak dokumen!
```

### 4. PyPDF (v3.17.0+)

**Apa itu PyPDF?**

Library Python untuk read dan manipulate PDF files.

**Mengapa PyPDF?**

| Feature | PyPDF | Alternatives |
|---------|-------|--------------|
| Pure Python | ‚úÖ Yes | ‚ùå PyPDF2, pdfplumber (deps) |
| Install | ‚úÖ pip install pypdf | üõ†Ô∏è Compile needed |
| PDF Support | ‚úÖ Most PDFs | ‚ö†Ô∏è Some PDFs only |
| Speed | ‚ö° Fast | üêå Varies |
| Maintained | ‚úÖ Active | ‚ö†Ô∏è Some abandoned |

**Cara Kerja:**

```python
import pypdf

# Open PDF
with open("document.pdf", 'rb') as file:
    pdf_reader = pypdf.PdfReader(file)

    # Get number of pages
    num_pages = len(pdf_reader.pages)

    # Iterate pages
    for page_num, page in enumerate(pdf_reader.pages, start=1):
        # Extract text from page
        text = page.extract_text()

        # Text contains all text elements on page
        print(f"Page {page_num}: {len(text)} characters")
```

**Limitations:**

1. **Image-based PDFs:**
   - PyPDF tidak bisa extract text dari scan/gambar
   - Perlu OCR (Tesseract) untuk itu

2. **Complex layouts:**
   - Column layouts bisa campur
   - Tables bisa rusak format

3. **Encrypted PDFs:**
   - Perlu password untuk buka

**Solusi untuk Complex PDFs:**

Aplikasi ini juga include pdfplumber sebagai alternatif:
```python
import pdfplumber

with pdfplumber.open("document.pdf") as pdf:
    for page in pdf.pages:
        text = page.extract_text()
        # pdfplumber lebih baik untuk tables
```

### 5. Watchdog (v3.0.0+)

**Apa itu Watchdog?**

Library untuk monitor file system events (create, modify, delete) secara real-time.

**Mengapa Perlu File Watching?**

Tanpa file watching:
```
User add new PDF ‚Üí Manual re-run indexing ‚Üí Tedious!
```

Dengan file watching:
```
User add new PDF ‚Üí Auto-detect ‚Üí Auto-index ‚Üí Easy!
```

**Cara Kerja:**

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class MyHandler(FileSystemEventHandler):
    def on_created(self, event):
        if event.src_path.endswith('.pdf'):
            print(f"New PDF detected: {event.src_path}")
            # Auto-index

    def on_modified(self, event):
        if event.src_path.endswith('.pdf'):
            print(f"PDF modified: {event.src_path}")
            # Re-index

    def on_deleted(self, event):
        if event.src_path.endswith('.pdf'):
            print(f"PDF deleted: {event.src_path}")
            # Remove from database

# Start watching
observer = Observer()
observer.schedule(MyHandler(), path="./data/pdfs", recursive=False)
observer.start()
```

**Debouncing:**

Problem: File besar ditulis dalam chunks ‚Üí multiple events!
```
PDF being written:
  Event 1: on_created (file created)
  Event 2: on_modified (chunk 1 written)
  Event 3: on_modified (chunk 2 written)
  ...
  Event N: on_modified (writing done)
```

Solution: Debouncing (wait 2 seconds after last event)
```python
import time

last_event_time = {}

def debounce(file_path):
    current_time = time.time()

    if file_path in last_event_time:
        time_since_last = current_time - last_event_time[file_path]
        if time_since_last < 2.0:  # Less than 2 seconds
            # Still writing, DON'T process yet
            last_event_time[file_path] = current_time
            return False

    # Process the file
    last_event_time[file_path] = current_time
    return True
```

### 6. Python-dotenv (v1.0.0+)

**Apa itu python-dotenv?**

Library untuk load environment variables dari `.env` file.

**Mengapa Perlu .env?**

Best practice: Pisahkan configuration dari code!

‚ùå **Bad:**
```python
# Hardcoded di code
PDF_FOLDER = "C:/Users/arif/Documents/pdfs"
CHUNK_SIZE = 800
```
Problems:
- Sulit change settings
- Berbeda per developer
- Security risk (credentials di code)

‚úÖ **Good:**
```python
# di .env file
PDF_FOLDER=./data/pdfs
CHUNK_SIZE=800

# di code
from dotenv import load_dotenv
import os

load_dotenv()  # Load dari .env
PDF_FOLDER = os.getenv("PDF_FOLDER")
```
Benefits:
- Easy change settings
- Per-developer config
- Secure (`.env` di `.gitignore`)

**Cara Kerja:**

```python
# .env file
EMBEDDING_MODEL=all-mpnet-base-v2
CHUNK_SIZE=800
DEBUG=true

# Python code
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()  # Reads .env file

# Access variables
model = os.getenv("EMBEDDING_MODEL")  # "all-mpnet-base-v2"
chunk = int(os.getenv("CHUNK_SIZE"))  # 800
debug = os.getenv("DEBUG") == "true"  # True

# With defaults
device = os.getenv("DEVICE", "cpu")  # "cpu" if not set
```

---

## Penjelasan Setiap File

### 1. src/mcp_server.py (345 lines)

**Role:** Otak dari aplikasi - orchestration & MCP tools

**Responsibilities:**
1. Initialize semua komponen
2. Provide MCP tools untuk Claude
3. Coordinate processing pipeline
4. Handle file watching events

**Struktur Code:**

```python
"""
MCP Server implementation untuk PDF Vector DB RAG System
"""

# ============= IMPORTS =============
import asyncio
import logging
from pathlib import Path
from typing import Optional
from fastmcp import FastMCP

from .config import Config
from .pdf_processor import PDFProcessor
from .embeddings import EmbeddingGenerator
from .vector_store import VectorStore
from .file_watcher import PDFWatcher
from .utils import format_source_citation

logger = logging.getLogger(__name__)

# ============= FASTMCP INITIALIZATION =============
mcp = FastMCP("PDF Vector DB")

# ============= GLOBAL COMPONENTS =============
# Initialized saat server startup
pdf_processor = None
embedding_generator = None
vector_store = None
file_watcher = None

# ============= HELPER FUNCTIONS =============

async def process_pdf_file(pdf_path: Path):
    """
    Process single PDF file: extract ‚Üí chunk ‚Üí embed ‚Üí store

    Args:
        pdf_path: Path ke PDF file
    """
    try:
        logger.info(f"Processing PDF: {pdf_path.name}")

        # 1. Extract & chunk
        result = pdf_processor.process_pdf(pdf_path)
        chunks = result['chunks']

        # 2. Generate embeddings
        chunks_with_embeddings = embedding_generator.embed_chunks(chunks)

        # 3. Extract embeddings
        embeddings = [chunk['embedding'] for chunk in chunks_with_embeddings]

        # 4. Store in vector database
        vector_store.add_chunks(chunks, embeddings)

        logger.info(f"Successfully indexed {len(chunks)} chunks")

    except Exception as e:
        logger.error(f"Error processing {pdf_path.name}: {e}")
        raise

async def index_existing_pdfs():
    """Index all PDFs in PDF_FOLDER yang belum diindex"""
    pdf_folder = Config.PDF_FOLDER

    if not pdf_folder.exists():
        logger.warning(f"PDF folder tidak ditemukan: {pdf_folder}")
        return

    # Get all PDF files
    pdf_files = list(pdf_folder.glob("*.pdf"))

    if not pdf_files:
        logger.info("No PDF files found untuk indexing")
        return

    logger.info(f"Indexing {len(pdf_files)} PDFs...")

    # Get list of already indexed documents
    indexed_docs = {doc['document'] for doc in vector_store.list_documents()}

    # Index each PDF
    for pdf_path in pdf_files:
        if pdf_path.name not in indexed_docs:
            await process_pdf_file(pdf_path)
        else:
            logger.info(f"Skipping {pdf_path.name} (already indexed)")

# ============= MCP TOOLS =============

@mcp.tool()
def query_documents(query: str, top_k: int = None,
                   document: Optional[str] = None) -> str:
    """
    Search melalui indexed PDF documents menggunakan natural language.

    Args:
        query: Natural language query
        top_k: Jumlah results (default: Config.DEFAULT_TOP_K)
        document: Filter by document name (optional)

    Returns:
        Formatted search results dengan citations
    """
    try:
        # 1. Generate query embedding
        query_embedding = embedding_generator.generate_embedding(query)

        # 2. Search vector store
        filter_dict = {"document": document} if document else None
        results = vector_store.query(
            query_embedding=query_embedding,
            top_k=top_k or Config.DEFAULT_TOP_K,
            filter_dict=filter_dict
        )

        # 3. Format results
        if not results['documents'][0]:
            return "Tidak ada hasil ditemukan."

        formatted = f"Ditemukan {len(results['documents'][0])} chunk relevan:\n\n"

        for idx, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        ), start=1):
            similarity = (1 - distance) * 100  # Convert distance to similarity %
            source = format_source_citation(metadata)

            formatted += f"--- Hasil {idx} ---\n"
            formatted += f"Sumber: {source}\n"
            formatted += f"Relevansi: {similarity:.1f}%\n\n"
            formatted += f"{doc}\n\n"

        return formatted

    except Exception as e:
        logger.error(f"Error in query_documents: {e}")
        return f"Error: {str(e)}"

@mcp.tool()
def list_documents() -> str:
    """
    List semua indexed PDF documents dengan statistics

    Returns:
        Formatted list of documents
    """
    try:
        documents = vector_store.list_documents()

        if not documents:
            return "Tidak ada dokumen yang diindex."

        result = f"Dokumen Terindex ({len(documents)}):\n\n"

        for doc in documents:
            result += f"- {doc['document']}: "
            result += f"{doc['num_pages']} halaman, "
            result += f"{doc['num_chunks']} chunks\n"

        return result

    except Exception as e:
        logger.error(f"Error in list_documents: {e}")
        return f"Error: {str(e)}"

@mcp.tool()
def get_document_info(document: str) -> str:
    """
    Get detailed info tentang specific document

    Args:
        document: Document name (e.g., "python-intro.pdf")

    Returns:
        Detailed document information
    """
    try:
        info = vector_store.get_document_info(document)

        if not info:
            return f"Dokumen '{document}' tidak ditemukan."

        result = f"Dokumen: {info['document']}\n"
        result += f"Halaman: {info['num_pages']}\n"
        result += f"Chunks: {info['num_chunks']}\n"
        result += f"Nomor halaman: {', '.join(map(str, info['pages']))}\n"

        return result

    except Exception as e:
        logger.error(f"Error in get_document_info: {e}")
        return f"Error: {str(e)}"

@mcp.tool()
async def reindex_document(document: str) -> str:
    """
    Manually trigger re-indexing untuk specific document

    Args:
        document: Document name (e.g., "python-intro.pdf")

    Returns:
        Success message
    """
    try:
        pdf_path = Config.PDF_FOLDER / document

        if not pdf_path.exists():
            return f"File '{document}' tidak ditemukan."

        # Delete old data
        vector_store.delete_by_document(document)

        # Re-index
        await process_pdf_file(pdf_path)

        return f"Successfully re-indexed '{document}'"

    except Exception as e:
        logger.error(f"Error in reindex_document: {e}")
        return f"Error: {str(e)}"

@mcp.tool()
def get_system_stats() -> str:
    """
    Get system statistics dan configuration info

    Returns:
        System statistics
    """
    try:
        stats = vector_store.get_stats()
        config = Config.get_summary()

        result = "=== Statistik Sistem ===\n"
        result += f"Total Dokumen: {stats['total_documents']}\n"
        result += f"Total Chunks: {stats['total_chunks']}\n\n"

        result += "=== Konfigurasi ===\n"
        for key, value in config.items():
            result += f"{key}: {value}\n"

        return result

    except Exception as e:
        logger.error(f"Error in get_system_stats: {e}")
        return f"Error: {str(e)}"

# ============= INITIALIZATION =============

def initialize():
    """Initialize all components saat server startup"""
    global pdf_processor, embedding_generator, vector_store, file_watcher

    try:
        logger.info("Initializing PDF Vector DB MCP Server...")

        # Validate config
        Config.validate()

        # Initialize components
        pdf_processor = PDFProcessor()
        embedding_generator = EmbeddingGenerator()
        vector_store = VectorStore()

        # Validate embedding model works
        if not embedding_generator.validate_connection():
            raise Exception("Failed to load embedding model")

        logger.info("PDF Vector DB MCP Server components initialized")

    except Exception as e:
        logger.error(f"Error during initialization: {e}")
        raise

# ============= MAIN =============

if __name__ == "__main__":
    # Initialize components
    initialize()

    # Run server
    mcp.run()
```

**Key Concepts:**

1. **Global Components:**
   ```python
   # Why global?
   # - Initialized once saat startup
   # - Digunakan oleh multiple tools
   # - Expensive to create (embedding model ~4 sec to load)

   pdf_processor = None
   embedding_generator = None
   vector_store = None
   file_watcher = None
   ```

2. **Async Functions:**
   ```python
   async def process_pdf_file(pdf_path):
       # Why async?
       # - Allows other operations while processing
       # - Non-blocking untuk file watcher
       # - Better responsiveness

       await long_operation()  # Don't block here
   ```

3. **FastMCP Tools:**
   ```python
   @mcp.tool()
   def query_documents(query: str, top_k: int = 5) -> str:
       # Decorator @mcp.tool() automatically:
       # 1. Register tool ke MCP server
       # 2. Generate JSON schema dari type hints
       # 3. Validate parameters
       # 4. Handle errors

       return result  # Simple string return!
   ```

---

### 2. src/config.py (100 lines)

**Role:** Configuration management dengan environment variables

**Design Pattern:** Singleton-like class dengan class methods

**Code:**

```python
"""
Configuration management untuk PDF Vector DB
"""
import os
import logging
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables dari .env
load_dotenv()

class Config:
    """
    Centralized configuration management

    Semua settings diambil dari environment variables dengan defaults.
    Menggunakan class methods untuk easy access tanpa instantiation.
    """

    # ============= EMBEDDING SETTINGS =============
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "all-mpnet-base-v2")
    EMBEDDING_DEVICE = os.getenv("EMBEDDING_DEVICE", "auto")  # cpu, cuda, auto

    # ============= PATH SETTINGS =============
    BASE_DIR = Path(__file__).parent.parent  # Project root
    PDF_FOLDER = Path(os.getenv("PDF_FOLDER", BASE_DIR / "data" / "pdfs"))
    CHROMA_DB_PATH = Path(os.getenv("CHROMA_DB_PATH", BASE_DIR / "data" / "chroma_db"))

    # ============= CHUNKING SETTINGS =============
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))  # Characters per chunk
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))  # Overlap size

    # ============= RETRIEVAL SETTINGS =============
    DEFAULT_TOP_K = int(os.getenv("DEFAULT_TOP_K", "5"))  # Default results
    COLLECTION_NAME = os.getenv("COLLECTION_NAME", "pdf_documents")

    # ============= LOGGING SETTINGS =============
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

    @classmethod
    def validate(cls):
        """
        Validate configuration dan create directories

        Raises:
            ValueError: If configuration invalid
        """
        # Validate chunk settings
        if cls.CHUNK_SIZE < 100:
            raise ValueError("CHUNK_SIZE harus >= 100")

        if cls.CHUNK_OVERLAP >= cls.CHUNK_SIZE:
            raise ValueError("CHUNK_OVERLAP harus < CHUNK_SIZE")

        # Create directories jika belum ada
        cls.PDF_FOLDER.mkdir(parents=True, exist_ok=True)
        cls.CHROMA_DB_PATH.mkdir(parents=True, exist_ok=True)

        logging.info(f"Configuration validated successfully")

    @classmethod
    def get_summary(cls) -> dict:
        """
        Get configuration summary untuk debugging

        Returns:
            Dictionary dengan semua settings
        """
        return {
            "Embedding Model": cls.EMBEDDING_MODEL,
            "Embedding Device": cls.EMBEDDING_DEVICE,
            "PDF Folder": str(cls.PDF_FOLDER),
            "ChromaDB Path": str(cls.CHROMA_DB_PATH),
            "Chunk Size": cls.CHUNK_SIZE,
            "Chunk Overlap": cls.CHUNK_OVERLAP,
            "Default Top-K": cls.DEFAULT_TOP_K,
            "Collection Name": cls.COLLECTION_NAME,
            "Log Level": cls.LOG_LEVEL
        }
```

**Key Concepts:**

1. **Environment Variables dengan Defaults:**
   ```python
   # os.getenv(key, default)
   # Jika env var tidak ada, gunakan default

   CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
   # Jika CHUNK_SIZE di .env ‚Üí gunakan itu
   # Jika tidak ada ‚Üí gunakan 800
   ```

2. **Path Management:**
   ```python
   BASE_DIR = Path(__file__).parent.parent
   # __file__ = config.py location
   # parent = src/
   # parent.parent = project root/

   PDF_FOLDER = BASE_DIR / "data" / "pdfs"
   # Path() object bisa di-join dengan /
   ```

3. **Type Conversion:**
   ```python
   # os.getenv() always returns string!
   CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "800"))
   # Convert string "800" ‚Üí int 800
   ```

4. **Class Methods:**
   ```python
   @classmethod
   def validate(cls):
       # cls = Config class
       # No need to create instance!
       cls.CHUNK_SIZE  # Access class attributes

   # Usage:
   Config.validate()  # No Config() needed!
   ```

---

### 3. src/pdf_processor.py (150 lines)

**Role:** Extract text dari PDF dan chunk into smaller pieces

**Pipeline:** PDF ‚Üí Pages ‚Üí Chunks dengan metadata

**Code:**

```python
"""
PDF processing: extraction, cleaning, dan chunking
"""
import logging
from pathlib import Path
from typing import List, Dict
import pypdf
import hashlib

from .config import Config
from .utils import split_text_with_overlap, create_chunk_id, get_file_hash

logger = logging.getLogger(__name__)

class PDFProcessor:
    """
    Process PDF files: extract text dan split into chunks
    """

    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):
        """
        Initialize PDF processor

        Args:
            chunk_size: Size per chunk (default: Config.CHUNK_SIZE)
            chunk_overlap: Overlap between chunks (default: Config.CHUNK_OVERLAP)
        """
        self.chunk_size = chunk_size or Config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or Config.CHUNK_OVERLAP

        logger.info(f"PDFProcessor initialized: chunk_size={self.chunk_size}, overlap={self.chunk_overlap}")

    def _clean_text(self, text: str) -> str:
        """
        Clean extracted text dari PDF

        Args:
            text: Raw text dari PDF

        Returns:
            Cleaned text
        """
        # Remove null characters
        text = text.replace('\x00', '')

        # Normalize whitespace
        text = ' '.join(text.split())

        return text

    def extract_text_from_pdf(self, pdf_path: Path) -> List[Dict]:
        """
        Extract text dari PDF page by page

        Args:
            pdf_path: Path ke PDF file

        Returns:
            List of page dictionaries:
            [
                {
                    'page_number': 1,
                    'text': "Page 1 content...",
                    'document': "filename.pdf"
                },
                ...
            ]
        """
        logger.info(f"Extracting text from: {pdf_path.name}")

        pages_data = []

        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = pypdf.PdfReader(file)

                logger.info(f"Processing {len(pdf_reader.pages)} pages from {pdf_path.name}")

                for page_num, page in enumerate(pdf_reader.pages, start=1):
                    # Extract text
                    text = page.extract_text()

                    # Clean text
                    text = self._clean_text(text)

                    # Skip empty pages
                    if not text.strip():
                        logger.warning(f"Page {page_num} is empty")
                        continue

                    pages_data.append({
                        'page_number': page_num,
                        'text': text,
                        'document': pdf_path.name
                    })

                logger.info(f"Extracted {len(pages_data)} non-empty pages")

        except Exception as e:
            logger.error(f"Error extracting from {pdf_path.name}: {e}")
            raise

        return pages_data

    def chunk_pages(self, pages_data: List[Dict]) -> List[Dict]:
        """
        Split pages into smaller chunks dengan overlap

        Args:
            pages_data: List of page dictionaries dari extract_text_from_pdf()

        Returns:
            List of chunk dictionaries:
            [
                {
                    'id': "doc.pdf::page_1::chunk_0",
                    'text': "Chunk content...",
                    'metadata': {
                        'document': "doc.pdf",
                        'page': 1,
                        'chunk_index': 0,
                        'total_chunks_on_page': 3
                    }
                },
                ...
            ]
        """
        logger.info(f"Chunking {len(pages_data)} pages...")

        all_chunks = []

        for page_data in pages_data:
            page_num = page_data['page_number']
            text = page_data['text']
            document = page_data['document']

            # Split text into chunks
            chunks = split_text_with_overlap(
                text,
                chunk_size=self.chunk_size,
                overlap=self.chunk_overlap
            )

            # Create chunk dictionaries dengan metadata
            for chunk_idx, chunk_text in enumerate(chunks):
                chunk_id = create_chunk_id(document, page_num, chunk_idx)

                chunk = {
                    'id': chunk_id,
                    'text': chunk_text,
                    'metadata': {
                        'document': document,
                        'page': page_num,
                        'chunk_index': chunk_idx,
                        'total_chunks_on_page': len(chunks)
                    }
                }

                all_chunks.append(chunk)

        logger.info(f"Created {len(all_chunks)} chunks")

        return all_chunks

    def process_pdf(self, pdf_path: Path) -> Dict:
        """
        Main processing pipeline: extract ‚Üí chunk

        Args:
            pdf_path: Path ke PDF file

        Returns:
            Processing result:
            {
                'document': "filename.pdf",
                'file_hash': "abc123...",
                'num_pages': 10,
                'num_chunks': 45,
                'chunks': [...]
            }
        """
        logger.info(f"Processing PDF: {pdf_path.name}")

        # 1. Extract text
        pages_data = self.extract_text_from_pdf(pdf_path)

        # 2. Chunk pages
        chunks = self.chunk_pages(pages_data)

        # 3. Calculate file hash untuk change detection
        file_hash = get_file_hash(pdf_path)

        result = {
            'document': pdf_path.name,
            'file_hash': file_hash,
            'num_pages': len(pages_data),
            'num_chunks': len(chunks),
            'chunks': chunks
        }

        logger.info(f"Processed {pdf_path.name}: {len(pages_data)} pages ‚Üí {len(chunks)} chunks")

        return result
```

**Key Concepts:**

1. **Page-by-Page Processing:**
   ```python
   # Why page-by-page?
   # - Memory efficient (don't load entire PDF)
   # - Preserve page numbers untuk citations
   # - Handle large PDFs (100+ pages)

   for page_num, page in enumerate(pdf_reader.pages):
       text = page.extract_text()
       # Process one page at a time
   ```

2. **Text Cleaning:**
   ```python
   def _clean_text(self, text: str):
       # Remove artifacts dari PDF extraction
       text = text.replace('\x00', '')  # Null chars
       text = ' '.join(text.split())    # Normalize whitespace
       return text
   ```

3. **Chunk ID Generation:**
   ```python
   # Hierarchical ID structure:
   # document.pdf::page_5::chunk_2
   #     ‚Üì           ‚Üì        ‚Üì
   #  filename    page num  chunk idx

   # Benefits:
   # - Easy to delete all chunks from a document
   # - Easy to identify source
   # - Unique across all documents
   ```

4. **File Hash untuk Change Detection:**
   ```python
   file_hash = get_file_hash(pdf_path)
   # MD5 hash of file content
   # If file changed ‚Üí hash different ‚Üí re-index needed
   ```

---

*[Dokumentasi dilanjutkan dengan penjelasan file lainnya...]*

---

## Alur Kerja Aplikasi

### A. Indexing Flow (PDF ‚Üí Vector Store)

```
Step 1: File Detection
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Trigger:
  ‚Ä¢ Server startup: scan data/pdfs/
  ‚Ä¢ File watcher: new PDF added

Action:
  Check if PDF already indexed ‚Üí Skip atau Process
```

```
Step 2: Text Extraction (pdf_processor.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  "document.pdf"
        ‚Üì
PDFProcessor.extract_text_from_pdf():
  1. Open PDF dengan pypdf
  2. For each page:
     - page.extract_text()
     - Clean text (remove artifacts)
     - Store page data
        ‚Üì
Output: [
  {page_number: 1, text: "...", document: "doc.pdf"},
  {page_number: 2, text: "...", document: "doc.pdf"},
  ...
]
```

```
Step 3: Chunking (pdf_processor.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Pages data
        ‚Üì
PDFProcessor.chunk_pages():
  1. For each page:
     - split_text_with_overlap(text, 800, 200)
       ‚Üí Try sentence boundary
       ‚Üí Fallback to word boundary
       ‚Üí Add overlap untuk context
     - Create chunk ID: "doc.pdf::page_1::chunk_0"
     - Attach metadata
        ‚Üì
Output: [
  {
    id: "doc.pdf::page_1::chunk_0",
    text: "Python is a programming...",
    metadata: {document: "doc.pdf", page: 1, chunk_index: 0}
  },
  ...
]
```

```
Step 4: Embedding Generation (embeddings.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Chunks dengan text
        ‚Üì
EmbeddingGenerator.embed_chunks():
  1. Extract texts: ["text1", "text2", ...]
  2. Model.encode(texts, batch_size=32):
     For each batch:
       - Tokenize text
       - Forward pass through transformer
       - Mean pooling
       - L2 normalization
       - Generate 768-dim vector
  3. Add embeddings ke chunks
        ‚Üì
Output: [
  {
    id: "doc.pdf::page_1::chunk_0",
    text: "Python is a programming...",
    metadata: {...},
    embedding: [0.234, -0.567, ..., 0.891]  # 768 floats
  },
  ...
]
```

```
Step 5: Vector Storage (vector_store.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Chunks dengan embeddings
        ‚Üì
VectorStore.add_chunks():
  1. Extract components:
     - ids: ["doc.pdf::page_1::chunk_0", ...]
     - documents: ["text", ...]
     - embeddings: [[0.1, 0.2, ...], ...]
     - metadatas: [{document: "doc.pdf", page: 1}, ...]

  2. ChromaDB.collection.add():
     - Store in SQLite: data/chroma_db/chroma.sqlite3
     - Build HNSW index untuk fast search
     - Persist to disk
        ‚Üì
Output: ‚úì Document indexed successfully!
```

### B. Query Flow (User Question ‚Üí Answer)

```
Step 1: User Query
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
User in Claude Desktop:
  "What are the main features of Python?"
        ‚Üì
Claude decides to use MCP tool:
  query_documents(
    query="What are the main features of Python?",
    top_k=5
  )
```

```
Step 2: Query Embedding (embeddings.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  "What are the main features of Python?"
        ‚Üì
EmbeddingGenerator.generate_embedding(query):
  1. Tokenize query
  2. Forward pass through model
  3. Mean pooling
  4. L2 normalize
        ‚Üì
Output: Query embedding
  [0.234, -0.567, 0.890, ..., 0.432]  # 768 floats
```

```
Step 3: Vector Search (vector_store.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Query embedding + top_k=5
        ‚Üì
VectorStore.query(query_embedding, top_k=5):
  1. ChromaDB performs cosine similarity:
     For each stored embedding:
       similarity = dot(query_vec, stored_vec)
     (Optimized dengan HNSW index)

  2. Sort by similarity
  3. Return top 5
        ‚Üì
Output: Search results
  {
    'ids': [["doc.pdf::page_12::chunk_1", ...]],
    'documents': [["Python features include...", ...]],
    'metadatas': [[{document: "python.pdf", page: 12}, ...]],
    'distances': [[0.234, 0.345, ...]]  # Lower = more similar
  }
```

```
Step 4: Result Formatting (mcp_server.py)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Search results
        ‚Üì
Format results:
  1. Convert distance ‚Üí similarity percentage:
     distance 0.234 ‚Üí similarity 76.6%

  2. Format source citation:
     {document: "python.pdf", page: 12}
     ‚Üí "python.pdf (Halaman 12)"

  3. Build response string:
     """
     Ditemukan 5 chunk relevan:

     --- Hasil 1 ---
     Sumber: python-intro.pdf (Halaman 12)
     Relevansi: 76.6%

     Python features include dynamic typing, automatic
     memory management, and extensive standard library...

     --- Hasil 2 ---
     Sumber: python-intro.pdf (Halaman 15)
     Relevansi: 65.5%

     Key advantages of Python are readability, large
     community support, and versatility...

     [...]
     """
        ‚Üì
Output: Formatted string ke Claude
```

```
Step 5: Claude Response
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Input:  Formatted results dari MCP tool
        ‚Üì
Claude menggunakan context untuk generate answer:
  "Berdasarkan dokumentasi, main features Python meliputi:

   1. Dynamic typing - tidak perlu declare variable types
   2. Automatic memory management dengan garbage collection
   3. Extensive standard library (batteries included)
   4. Highly readable syntax
   5. Large community support

   Features ini membuat Python versatile dan beginner-friendly
   namun tetap powerful untuk aplikasi advanced.

   (Sumber: python-intro.pdf, Halaman 12-15)"
```

---

## Design Patterns

### 1. Layered Architecture (3-Tier)

**Definisi:** Memisahkan aplikasi menjadi 3 layer dengan responsibilities berbeda.

**Layers:**
```
Presentation Layer
  ‚Üì (hanya boleh panggil Business Logic)
Business Logic Layer
  ‚Üì (hanya boleh panggil Data Layer)
Data Layer
```

**Keuntungan:**
- ‚úÖ **Separation of concerns**: Each layer fokus pada responsibility-nya
- ‚úÖ **Easy to test**: Mock layer di bawahnya
- ‚úÖ **Maintainable**: Change satu layer tidak impact layer lain
- ‚úÖ **Scalable**: Bisa deploy layers ke servers berbeda

**Contoh di Aplikasi Ini:**
```python
# Presentation Layer: query_documents() tool
@mcp.tool()
def query_documents(query: str):
    # Hanya orchestrate, tidak ada logic detail
    embedding = embedding_generator.generate_embedding(query)  # Business Logic
    results = vector_store.query(embedding)                     # Data Layer
    return format_results(results)

# Business Logic Layer: EmbeddingGenerator
class EmbeddingGenerator:
    def generate_embedding(self, text):
        # Business logic: how to generate embedding
        return self.model.encode(text)

# Data Layer: VectorStore
class VectorStore:
    def query(self, embedding):
        # Data access: how to query database
        return self.collection.query(query_embeddings=[embedding])
```

### 2. Dependency Injection

**Definisi:** Pass dependencies dari luar, jangan create di dalam class.

**Why?**
- Easy untuk testing (inject mock objects)
- Loose coupling antar components
- Flexible configuration

**Example:**

‚ùå **Tight Coupling (Bad):**
```python
class PDFProcessor:
    def __init__(self):
        self.chunk_size = 800  # Hardcoded!
        self.chunk_overlap = 200  # Hardcoded!
```
Problem: Sulit change settings, sulit test dengan values berbeda

‚úÖ **Dependency Injection (Good):**
```python
class PDFProcessor:
    def __init__(self, chunk_size=None, chunk_overlap=None):
        # Inject or use defaults
        self.chunk_size = chunk_size or Config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or Config.CHUNK_OVERLAP

# Usage:
processor = PDFProcessor(chunk_size=1000, chunk_overlap=300)  # Custom
processor = PDFProcessor()  # Use defaults from Config

# Testing:
test_processor = PDFProcessor(chunk_size=100, chunk_overlap=20)  # Test values
```

### 3. Single Responsibility Principle (SRP)

**Definisi:** Each class should have ONE clear responsibility.

**Example di Aplikasi:**

```python
# ‚úÖ GOOD: Each class has ONE job

PDFProcessor
  Responsibility: PDF extraction & chunking
  NOT responsible for: embeddings, storage, file watching

EmbeddingGenerator
  Responsibility: Generate embeddings
  NOT responsible for: PDF processing, storage

VectorStore
  Responsibility: Database operations
  NOT responsible for: embeddings, PDF processing

PDFWatcher
  Responsibility: File system monitoring
  NOT responsible for: PDF processing, storage
```

**Why SRP Important?**
- Easy to understand (small, focused classes)
- Easy to test (test one thing at a time)
- Easy to change (change one thing, impact minimal)

**Bad Example (Violation):**
```python
# ‚ùå BAD: One class doing too much!
class PDFManager:
    def extract_pdf(self):
        # Extract text
        pass

    def generate_embeddings(self):
        # Generate embeddings
        pass

    def store_in_database(self):
        # Store in ChromaDB
        pass

    def watch_files(self):
        # Watch for new files
        pass

# Problems:
# - Hard to understand (too many responsibilities)
# - Hard to test (test everything together?)
# - Hard to change (any change impacts everything)
```

### 4. Pipeline Pattern

**Definisi:** Data processing sebagai series of stages.

**Structure:**
```
Input ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output
```

**Example:**
```python
# Pipeline: PDF ‚Üí Indexed Document

pdf_file
  ‚Üì Stage 1: Extract
pages_data = pdf_processor.extract_text_from_pdf(pdf_file)
  ‚Üì Stage 2: Chunk
chunks = pdf_processor.chunk_pages(pages_data)
  ‚Üì Stage 3: Embed
chunks_with_emb = embedding_generator.embed_chunks(chunks)
  ‚Üì Stage 4: Store
vector_store.add_chunks(chunks_with_emb)
  ‚Üì
indexed_document
```

**Benefits:**
- Each stage independent dan reusable
- Easy to add/remove stages
- Easy to test individual stages
- Clear data flow

### 5. Observer Pattern (File Watching)

**Definisi:** Observer "watch" Subject dan react saat events occur.

**Structure:**
```
Subject (File System)
  ‚Üì notify
Observer (PDFWatcher)
  ‚Üì react
Event Handlers (process_pdf_file, etc.)
```

**Example:**
```python
# Subject: File System
# Observer: PDFWatcher
# Events: created, modified, deleted

class PDFWatcher:
    def start(self, on_created, on_modified, on_deleted):
        # Register callbacks
        self.event_handler = PDFFileHandler(
            on_created=on_created,
            on_modified=on_modified,
            on_deleted=on_deleted
        )

        # Start watching
        self.observer = Observer()
        self.observer.schedule(self.event_handler, path)
        self.observer.start()

# Usage:
watcher.start(
    on_created=lambda path: process_pdf_file(path),
    on_modified=lambda path: reindex_pdf(path),
    on_deleted=lambda path: remove_from_db(path)
)
```

**Benefits:**
- Decoupled (file monitoring terpisah dari processing)
- Event-driven (react to changes automatically)
- Flexible (easy to add new event handlers)

### 6. Fa√ßade Pattern (MCP Server)

**Definisi:** Provide simple interface untuk complex subsystem.

**Structure:**
```
Simple Interface (MCP Tools)
  ‚Üì hide complexity
Complex Subsystem (PDFProcessor, EmbeddingGenerator, VectorStore)
```

**Example:**
```python
# Fa√ßade: query_documents() tool
@mcp.tool()
def query_documents(query: str) -> str:
    # Simple interface untuk user
    # Behind the scenes: multiple complex operations

    # 1. EmbeddingGenerator (complex)
    query_embedding = embedding_generator.generate_embedding(query)

    # 2. VectorStore (complex)
    results = vector_store.query(query_embedding)

    # 3. Result formatting (complex)
    formatted = format_results(results)

    # User only sees: simple function call!
    return formatted

# User perspective:
# query_documents("What is Python?")
#   ‚Üì (magic happens internally)
# Returns formatted results
```

**Benefits:**
- Simple interface untuk complex operations
- Hide implementation details
- Easy to change internals tanpa break API

---

## Integrasi Komponen

### FastMCP ‚Üî Business Logic

**How They Work Together:**

```python
# FastMCP handles:
# 1. MCP protocol communication
# 2. Tool registration
# 3. Parameter validation
# 4. Error handling
# 5. Serialization/deserialization

@mcp.tool()
def query_documents(query: str, top_k: int = 5) -> str:
    # You focus on business logic only!
    embedding = embedding_generator.generate_embedding(query)
    results = vector_store.query(embedding, top_k)
    return format_results(results)

# FastMCP automatically:
# - Validates query is string
# - Validates top_k is integer
# - Converts return string to MCP format
# - Handles errors gracefully
```

**Benefits:**
- Developer fokus pada logic, bukan protocol details
- Type-safe dengan validation otomatis
- Clean code dengan decorators

### SentenceTransformers ‚Üî ChromaDB

**Perfect Match:**

```python
# 1. SentenceTransformers produces normalized embeddings
embedding = model.encode(text)
# Output: L2-normalized vector (length = 1)

# 2. ChromaDB configured for cosine similarity
collection = client.get_or_create_collection(
    metadata={"hnsw:space": "cosine"}
)

# 3. Cosine similarity of normalized vectors = dot product
# ChromaDB efficiently computes this dengan HNSW index

# Why perfect?
# - No need to normalize again
# - Dot product is faster than cosine computation
# - Both optimized for semantic search
```

**Math Behind:**

```
Cosine Similarity:
  cos(A, B) = dot(A, B) / (||A|| * ||B||)

If A and B are normalized (||A|| = ||B|| = 1):
  cos(A, B) = dot(A, B) / (1 * 1) = dot(A, B)

So:
  Cosine similarity = Dot product (for normalized vectors)

ChromaDB can compute dot product VERY fast!
```

### PyPDF ‚Üî Chunking Strategy

**Seamless Integration:**

```python
# PyPDF extracts page-by-page
for page in pdf_reader.pages:
    text = page.extract_text()
    # Memory efficient: process one page at a time

# Chunking preserves page info
chunks = chunk_pages(pages_data)
# Each chunk knows its original page

# Benefits:
# 1. Citations dengan page numbers
# 2. Memory efficient untuk large PDFs
# 3. Easy to re-process specific pages
```

### Watchdog ‚Üî Async Processing

**Event-Driven Architecture:**

```python
# 1. Watchdog detects events (synchronous)
def on_created(event):
    # File created!
    path = Path(event.src_path)

# 2. Trigger async processing (non-blocking)
watcher.start(
    on_created=lambda path: asyncio.create_task(process_pdf_file(path))
)

# 3. Processing runs asynchronously
async def process_pdf_file(path):
    # Long operation, but doesn't block watcher
    await index_pdf(path)

# Benefits:
# - Watcher continues monitoring
# - Multiple files can be processed concurrently
# - Responsive system
```

---

## Kesimpulan

Aplikasi PDF Vector DB MCP Server ini adalah implementasi modern dari sistem RAG (Retrieval-Augmented Generation) yang:

1. **100% Lokal & Gratis**
   - Tidak perlu API key
   - Tidak perlu internet (after setup)
   - Privacy-friendly (data tidak keluar dari komputer)

2. **Teknologi Terkini**
   - FastMCP untuk MCP integration
   - Sentence-Transformers untuk embeddings lokal
   - ChromaDB untuk efficient vector search
   - Watchdog untuk real-time monitoring

3. **Best Practices**
   - Layered architecture untuk maintainability
   - Design patterns untuk clean code
   - Type hints untuk safety
   - Comprehensive logging untuk debugging

4. **Production-Ready**
   - Error handling yang robust
   - Configuration management
   - Async processing untuk responsiveness
   - Auto-indexing untuk convenience

Aplikasi ini menunjukkan bagaimana berbagai teknologi modern dapat diintegrasikan untuk membuat sistem yang powerful namun tetap sederhana digunakan.

---

**Catatan Penting untuk Pengajaran:**

1. **Fokus pada Konsep**, bukan hanya code
2. **Gunakan Analogi** untuk konsep kompleks
3. **Hands-on Labs** sangat penting
4. **Encourage Questions** - tidak ada pertanyaan bodoh
5. **Build Incrementally** - mulai dari simple, tambah complexity gradually

Semoga dokumentasi ini membantu dalam mengajar murid-murid Anda! üöÄ
